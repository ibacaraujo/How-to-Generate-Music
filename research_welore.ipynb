{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMDEVWOBmm4zCZ4161mgqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibacaraujo/How-to-Generate-Music/blob/master/research_welore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research, rank reduction. Base code, WeLore. For study and progress."
      ],
      "metadata": {
        "id": "p0G-LmftgKpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries."
      ],
      "metadata": {
        "id": "3WAboIg4jE1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru\n",
        "!pip install wandb\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "tXEW8ZfHjGiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries."
      ],
      "metadata": {
        "id": "drpFcmBqjLpe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vQEq52DW87_I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from importlib.metadata import version\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta\n",
        "\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "GsNCwJMB9RWo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from loguru import logger\n",
        "import wandb"
      ],
      "metadata": {
        "id": "3Le8001JneHb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils.\n",
        "\n",
        "`from utils import *.`"
      ],
      "metadata": {
        "id": "jq3_xSkFgGyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "\n",
        "def save_dict(item, filename):\n",
        "    with open(filename, 'wb') as handle:\n",
        "        pickle.dump(item, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def format_subject(subject):\n",
        "    l = subject.split(\"_\")\n",
        "    s = \"\"\n",
        "    for entry in l:\n",
        "        s += \" \" + entry\n",
        "    return s\n",
        "\n",
        "def shuffleDict(d):\n",
        "  keys = list(d.keys())\n",
        "  random.shuffle(keys)\n",
        "  [(key, d[key]) for key in keys]\n",
        "  random.shuffle(keys)\n",
        "  [(key, d[key]) for key in keys]\n",
        "  random.shuffle(keys)\n",
        "  keys = [(key, d[key]) for key in keys]\n",
        "  #keys = d(keys)\n",
        "  return dict(keys)\n",
        "\n",
        "def fix_seed(seed):\n",
        "    # random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def format_example(df, idx, include_answer=True):\n",
        "    prompt = df.iloc[idx, 0]\n",
        "    k = df.shape[1] - 2\n",
        "    for j in range(k):\n",
        "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j + 1])\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    if include_answer:\n",
        "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def gen_prompt(train_df, subject, k=-1):\n",
        "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
        "        format_subject(subject)\n",
        "    )\n",
        "    if k == -1:\n",
        "        k = train_df.shape[0]\n",
        "    for i in range(k):\n",
        "        prompt += format_example(train_df, i)\n",
        "    return prompt\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(args, subject, model, tokenizer, dev_df, test_df, f):\n",
        "    cors = []\n",
        "    all_probs = []\n",
        "    answers = choices[: test_df.shape[1] - 2]\n",
        "\n",
        "    for i in range(test_df.shape[0]):\n",
        "        # get prompt and make sure it fits\n",
        "        k = args.ntrain\n",
        "        prompt_end = format_example(test_df, i, include_answer=False)\n",
        "        train_prompt = gen_prompt(dev_df, subject, k)\n",
        "        prompt = train_prompt + prompt_end\n",
        "        # print(prompt)\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
        "\n",
        "        while input_ids.shape[-1] > 2048:\n",
        "            k -= 1\n",
        "            train_prompt = gen_prompt(dev_df, subject, k)\n",
        "            prompt = train_prompt + prompt_end\n",
        "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
        "\n",
        "        label = test_df.iloc[i, test_df.shape[1] - 1]\n",
        "\n",
        "        generate_ids = model.generate(input_ids, max_length=len(input_ids[0]) + 1)\n",
        "        output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "        pred = output[-1:]\n",
        "        print(label, pred)\n",
        "\n",
        "        cor = pred == label\n",
        "        cors.append(cor)\n",
        "\n",
        "    acc = np.mean(cors)\n",
        "    cors = np.array(cors)\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    print(\"Average accuracy {:.3f} - {}\".format(acc, subject), file=f)\n",
        "    f.flush()\n",
        "\n",
        "    return cors, acc, all_probs\n",
        "\n",
        "\n",
        "def uniform_rank_pruning(args, pruning_ratio, layers_singular_value, logger):\n",
        "    total_rank, pruned_rank = 0, 0\n",
        "    rank_pruning = {}\n",
        "    for index in range(0, len(layers_singular_value)):\n",
        "        layer = layers_singular_value[index]\n",
        "        subset = list(layer.keys())\n",
        "        rank_pruning[index] = {}\n",
        "        for name in subset:\n",
        "            _data = layer[name].clone().cpu().numpy()\n",
        "            rank_pruning[index][name] = int(pruning_ratio * len(_data))\n",
        "            total_rank += len(_data)\n",
        "            pruned_rank += rank_pruning[index][name]\n",
        "    logger.info(f\"Attempted Rank Reduction: {(pruned_rank/total_rank)* 100:.3f} %\")\n",
        "    return rank_pruning\n",
        "\n",
        "def adaptive_rank_pruning(args, pruning_ratio, layers_singular_value, logger):\n",
        "    logger.info(f\"Using the mean threolding\\nsum(_data < args.rank_thresold = {args.rank_thresold})\\n\\n\")\n",
        "    total_rank, pruned_rank = 0, 0\n",
        "    rank_pruning = {}\n",
        "    for index in range(0, len(layers_singular_value)):\n",
        "        layer = layers_singular_value[index]\n",
        "        subset = list(layer.keys())\n",
        "        rank_pruning[index] = {}\n",
        "        for name in subset:\n",
        "            data = layer[name].clone().cpu().numpy()\n",
        "            _data = (data-min(data))/(max(data)-min(data))\n",
        "            rank_pruning[index][name] = sum(_data < args.rank_thresold) # Rank which will be pruned\n",
        "            total_rank += len(_data)\n",
        "            pruned_rank += rank_pruning[index][name]\n",
        "    logger.info(f\"Attempted Rank Reduction: {(pruned_rank/total_rank)* 100:.3f} %\")\n",
        "    return rank_pruning\n",
        "\n",
        "def uniform_rank_pruning_exp2(args, pruning_ratio, layers_singular_value, file_name):\n",
        "    total_rank, pruned_rank = 0, 0\n",
        "    rank_pruning = {}\n",
        "    prune_layers = [15, 22, 25, 27]\n",
        "    for index in range(0, len(layers_singular_value)):\n",
        "        layer = layers_singular_value[index]\n",
        "        subset = list(layer.keys())\n",
        "        rank_pruning[index] = {}\n",
        "        for name in subset:\n",
        "            _data = layer[name].clone().cpu().numpy()\n",
        "            if index in prune_layers:\n",
        "                rank_pruning[index][name] = int(pruning_ratio * len(_data))\n",
        "            else:\n",
        "                rank_pruning[index][name] = 0\n",
        "            total_rank += len(_data)\n",
        "            pruned_rank += rank_pruning[index][name]\n",
        "            print(f\"layer{index}.{name} rank reduction: \\t\\t{(rank_pruning[index][name]/len(_data))* 100:.3f} %\", file=file_name, flush=True)\n",
        "    print(f\"Rank Reduction: {(pruned_rank/total_rank)* 100:.3f} %\", file=file_name, flush=True)\n",
        "    return rank_pruning\n",
        "\n",
        "def weight_thresold_rank_pruning(args, layers_singular_value, file_name):\n",
        "    \"\"\"\n",
        "    Given a rank thresold, normalize the singular values and prune each layer under the rank_thresold\n",
        "    \"\"\"\n",
        "    print(f\"Using the mean threolding\\nsum(_data < args.rank_thresold = {args.rank_thresold})\\n\\n\", file=file_name, flush=True)\n",
        "    total_rank, pruned_rank = 0, 0\n",
        "    rank_pruning = {}\n",
        "    for index in range(0, len(layers_singular_value)):\n",
        "        layer = layers_singular_value[index]\n",
        "        subset = list(layer.keys())\n",
        "        rank_pruning[index] = {}\n",
        "        for name in subset:\n",
        "            data = layer[name].clone().cpu().numpy()\n",
        "            _data = (data-min(data))/(max(data)-min(data))\n",
        "            rank_pruning[index][name] = sum(_data < args.rank_thresold) # Rank which will be pruned\n",
        "            total_rank += len(_data)\n",
        "            pruned_rank += rank_pruning[index][name]\n",
        "            print(f\"layer{index}.{name} rank reduction: \\t\\t{(rank_pruning[index][name]/len(_data))* 100:.3f} %\", file=file_name, flush=True)\n",
        "    print(f\"\\n\\n Total Rank Reduction: {(pruned_rank/total_rank)* 100:.3f} %\", file=file_name, flush=True)\n",
        "    return rank_pruning"
      ],
      "metadata": {
        "id": "HfgUrH4agIEt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lib.\n",
        "\n",
        "```\n",
        "from lib.rank_reduction import do_rank_reduction\n",
        "from lib.rank_utils import rank_analysis_weight\n",
        "\n",
        "from lib.eval import eval_ppl\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FhutcbEmhDm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data utils."
      ],
      "metadata": {
        "id": "zBciVi4AiV6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "\n",
        "# Wrapper for tokenized input IDs\n",
        "class TokenizerWrapper:\n",
        "    def __init__(self, input_ids):\n",
        "        self.input_ids = input_ids\n",
        "\n",
        "def get_c4(nsamples, seed, seqlen, tokenizer):\n",
        "    # Load train and validation datasets\n",
        "    traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, ignore_verifications=True)[\"train\"]\n",
        "    valdata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, ignore_verifications=True)[\"validation\"]\n",
        "\n",
        "    # Generate samples from training set\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] >= seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    # Prepare validation dataset\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "    return trainloader, valenc\n",
        "\n",
        "# Load and process wikitext2 dataset\n",
        "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
        "    # Load train and test datasets\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    # Encode datasets\n",
        "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    # Generate samples from training set\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "\n",
        "def shuffleDict(d):\n",
        "    keys = list(d.keys())\n",
        "    random.shuffle(keys)\n",
        "    [(key, d[key]) for key in keys]\n",
        "    random.shuffle(keys)\n",
        "    [(key, d[key]) for key in keys]\n",
        "    random.shuffle(keys)\n",
        "    keys = [(key, d[key]) for key in keys]\n",
        "    #keys = d(keys)\n",
        "    return dict(keys)\n",
        "\n",
        "def fix_seed(seed):\n",
        "    # random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "XiigI1zWiXaI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LowRankLayer."
      ],
      "metadata": {
        "id": "_XNPvmJWjdwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LowRankLayer(nn.Module):\n",
        "    \"\"\"given a linear layer find low rank decomposition\"\"\"\n",
        "    def __init__(self, desired_rank, weight, require_grad=True):\n",
        "        super().__init__()\n",
        "        self.rank = desired_rank\n",
        "\n",
        "\n",
        "        results = torch.svd(weight)\n",
        "        U = results[0][:, :desired_rank]\n",
        "        S = results[1][:desired_rank]\n",
        "        V = results[2][:, :desired_rank]\n",
        "\n",
        "        self.U = nn.Linear(desired_rank, U.shape[0], bias=False).to(weight.device)\n",
        "        self.V = nn.Linear(V.shape[0], desired_rank, bias=False).to(weight.device)\n",
        "\n",
        "        self.U.weight.data = U.mul(S.sqrt()).to(torch.bfloat16).contiguous()\n",
        "        self.V.weight.data = V.t().mul(S.sqrt().view(-1, 1)).to(torch.bfloat16).contiguous()\n",
        "\n",
        "        if require_grad == False:\n",
        "            self.U.weight.requires_grad = False\n",
        "            self.V.weight.requires_grad = False\n",
        "        else:\n",
        "            self.U.weight.requires_grad = True\n",
        "            self.V.weight.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.U(self.V(x.to(torch.bfloat16)))\n",
        "        return output\n",
        "\n",
        "\n",
        "class LowRankLayerEval(nn.Module):\n",
        "    \"\"\"given a linear layer find low rank decomposition\"\"\"\n",
        "    def __init__(self, desired_rank, weight, require_grad=True):\n",
        "        super().__init__()\n",
        "        self.rank = desired_rank\n",
        "        self.U = nn.Linear(desired_rank, weight.shape[0], bias=False, dtype=torch.bfloat16).to(weight.device)\n",
        "        self.V = nn.Linear(weight.shape[1], desired_rank, bias=False, dtype=torch.bfloat16).to(weight.device)\n",
        "\n",
        "        if require_grad == False:\n",
        "            self.U.weight.requires_grad = False\n",
        "            self.V.weight.requires_grad = False\n",
        "        else:\n",
        "            self.U.weight.requires_grad = True\n",
        "            self.V.weight.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.U(self.V(x.to(torch.bfloat16)))\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "h2HsaE8Pjgls"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### rank reduction."
      ],
      "metadata": {
        "id": "lQaeTOW0hlV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import heapq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from .data_utils import get_c4, get_wikitext2\n",
        "#from .LowRankLayer import LowRankLayer, LowRankLayerEval\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, tokenizer)\n",
        "    if \"c4\" in name:\n",
        "        return get_c4(nsamples, seed, seqlen, tokenizer)\n",
        "\n",
        "def find_layers(module, layers=[nn.Linear], name=''):\n",
        "    \"\"\"\n",
        "    Recursively find the layers of a certain type in a module.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): PyTorch module.\n",
        "        layers (list): List of layer types to find.\n",
        "        name (str): Name of the module.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of layers of the given type(s) within the module.\n",
        "    \"\"\"\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "def do_rank_reduction(args, model, tokenizer, rank_pruning, min_ratio, logger = None, load_only = False):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "    reduced_rank, total_rank = 0, 0\n",
        "\n",
        "    logger.info(\"*************** Pruning Model Started ***************\")\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i]\n",
        "        attention = getattr(layer, 'self_attn')\n",
        "        for key, module in attention.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'self_attn.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) > min_ratio:\n",
        "                    if load_only is False:   l = LowRankLayer(k, module.weight.to(torch.float32), True)\n",
        "                    else: l = LowRankLayerEval(k, module.weight.to(torch.float32), True)\n",
        "                    setattr(attention, key, l)\n",
        "                    del module\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                else:\n",
        "                    k = rank\n",
        "                    module.weight.requires_grad = False\n",
        "\n",
        "                total_rank += rank\n",
        "                logger.info(f\"layer.{i}.{name:50} Desired/Total: {k}/{rank} ({(k/rank * 100):2f} %)\")\n",
        "\n",
        "        mlp = getattr(layer, 'mlp')\n",
        "\n",
        "        for key, module in mlp.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'mlp.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) > min_ratio:\n",
        "                    if load_only is False:   l = LowRankLayer(k, module.weight.to(torch.float32), True)\n",
        "                    else: l = LowRankLayerEval(k, module.weight.to(torch.float32), True)\n",
        "                    setattr(mlp, key, l)\n",
        "                    del module\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                else:\n",
        "                    k = rank\n",
        "                    module.weight.requires_grad = False\n",
        "\n",
        "                total_rank += rank\n",
        "                logger.info(f\"layer.{i}.{name:50} Desired/Total: {k}/{rank}  ({(k/rank * 100):2f} %)\")\n",
        "\n",
        "        import gc; gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    logger.info(\"*************** Pruning Model Completed ***************\")\n",
        "    return (reduced_rank, total_rank)\n",
        "\n",
        "def do_low_rank(weight, desired_rank, debug=False):\n",
        "\n",
        "    results = torch.svd(weight)\n",
        "    U = results[0][:, :desired_rank]\n",
        "    S = results[1][:desired_rank]\n",
        "    V = results[2][:, :desired_rank]\n",
        "\n",
        "    weight_approx = U @ torch.diag(S) @ V.T\n",
        "    return weight_approx\n",
        "\n",
        "def do_rank_reduction_merge(args, model, tokenizer, rank_pruning, min_ratio, logger = None):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "    reduced_rank, total_rank = 0, 0\n",
        "\n",
        "    logger.info(\"*************** Pruning Model Started ***************\")\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i]\n",
        "        attention = getattr(layer, 'self_attn')\n",
        "        for key, module in attention.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'self_attn.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "\n",
        "                if (rank_pruning[i][name] / rank) > min_ratio:\n",
        "                    _W = module.weight.clone().data.to(torch.float32)\n",
        "                    _W_approx = do_low_rank(_W, k)\n",
        "                    module.weight.data = _W_approx.to(torch.bfloat16)\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                else:\n",
        "                    k = rank\n",
        "                    module.weight.requires_grad = False\n",
        "                total_rank += rank\n",
        "                logger.info(f\"layer.{i}.{name:40} Desired/Total: {k}/{rank}\")\n",
        "\n",
        "        mlp = getattr(layer, 'mlp')\n",
        "\n",
        "        for key, module in mlp.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'mlp.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) > min_ratio:\n",
        "                    _W = module.weight.clone().data.to(torch.float32)\n",
        "                    _W_approx = do_low_rank(_W, k)\n",
        "                    module.weight.data = _W_approx.to(torch.bfloat16)\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                else:\n",
        "                    k = rank\n",
        "                    module.weight.requires_grad = False\n",
        "\n",
        "                total_rank += rank\n",
        "                logger.info(f\"layer.{i}.{name:40} Desired/Total: {k}/{rank}\")\n",
        "\n",
        "    logger.info(\"*************** Pruning Model Completed ***************\")\n",
        "    return (reduced_rank, total_rank)\n",
        ""
      ],
      "metadata": {
        "id": "BO6V9sZshbcX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### rank utils."
      ],
      "metadata": {
        "id": "2rBQRUm4hxzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import heapq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from .data_utils import get_c4, get_wikitext2\n",
        "#from .LowRankLayer import LowRankLayer, LowRankLayerEval\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, tokenizer)\n",
        "    if \"c4\" in name:\n",
        "        return get_c4(nsamples, seed, seqlen, tokenizer)\n",
        "\n",
        "def find_layers(module, layers=[nn.Linear], name=''):\n",
        "    \"\"\"\n",
        "    Recursively find the layers of a certain type in a module.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): PyTorch module.\n",
        "        layers (list): List of layer types to find.\n",
        "        name (str): Name of the module.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of layers of the given type(s) within the module.\n",
        "    \"\"\"\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_calibration_input(model, dataloader, device):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    # dev = model.hf_device_map[\"model.embed_tokens\"]\n",
        "    if \"model.embed_tokens\" in model.hf_device_map:\n",
        "        device = model.hf_device_map[\"model.embed_tokens\"]\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=device)\n",
        "    inps.requires_grad = False\n",
        "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            cache['position_ids'] = kwargs['position_ids']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for batch in dataloader:\n",
        "        try:\n",
        "            model(batch[0].to(device))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask']\n",
        "    position_ids = cache['position_ids']\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "    return inps, outs, attention_mask, position_ids\n",
        "\n",
        "def rank_analysis_weight(args, model, tokenizer, device):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    layers_singular_value = {}\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "\n",
        "        layers_singular_value[i] = {}\n",
        "        # Perform Singular Value Decomposition (SVD)\n",
        "        for name in subset:\n",
        "            W = subset[name].weight.data\n",
        "            _, singular_values, _ = torch.svd(W.to(torch.float32))\n",
        "            layers_singular_value[i][name] = singular_values\n",
        "\n",
        "    return layers_singular_value\n",
        "\n",
        "def get_singular_values(args, model):\n",
        "    layers = model.model.layers\n",
        "    layers_singular_value = {}\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "\n",
        "\n",
        "        # Perform Singular Value Decomposition (SVD)\n",
        "        for name in subset:\n",
        "            W = subset[name].weight.data\n",
        "            _, singular_values, _ = torch.svd(W.to(torch.float32))\n",
        "            layers_singular_value[f\"layer.{i}.{name}\"] = singular_values\n",
        "\n",
        "    return layers_singular_value\n",
        "\n",
        "\n",
        "def get_grad_singular_values(args, model):\n",
        "    layers = model.model.layers\n",
        "    layers_singular_value = {}\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "\n",
        "\n",
        "        # Perform Singular Value Decomposition (SVD)\n",
        "        for name in subset:\n",
        "            W = subset[name].weight.grad\n",
        "            _, singular_values, _ = torch.svd(W.to(torch.float32))\n",
        "            layers_singular_value[f\"layer.{i}.{name}\"] = singular_values\n",
        "\n",
        "    return layers_singular_value\n",
        "\n",
        "def do_low_rank(weight, desired_rank, debug=False):\n",
        "\n",
        "    results = torch.svd(weight)\n",
        "    U = results[0][:, :desired_rank]\n",
        "    S = results[1][:desired_rank]\n",
        "    V = results[2][:, :desired_rank]\n",
        "\n",
        "    loss = torch.nn.L1Loss()\n",
        "    if debug:\n",
        "        print(f\"Shape is {weight.shape} and shape is {weight.dtype} => desired rank {desired_rank}\")\n",
        "\n",
        "    weight_approx = U @ torch.diag(S) @ V.T\n",
        "\n",
        "    if debug:\n",
        "        print(f\"New matrix has shape {weight_approx.shape}\")\n",
        "\n",
        "    assert weight_approx.shape[0] == weight.shape[0] and weight_approx.shape[1] == weight.shape[1]\n",
        "    weight_approx = torch.nn.Parameter(weight_approx)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        error = loss(weight, weight_approx)\n",
        "    return weight_approx, error\n",
        "\n",
        "def rank_reduction_weight(args, model, tokenizer, rank_pruning, device):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "    layers_singular_value = {}\n",
        "\n",
        "    for i in tqdm(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "\n",
        "        for name in subset:\n",
        "            W = subset[name].weight.data\n",
        "            k = min(W.shape[0], W.shape[1]) - rank_pruning[i][name]\n",
        "            approx_w, error = do_low_rank(W.to(torch.float32), k, True)\n",
        "            print(f\"layer.{i}.{name} ({k}): {error}\")\n",
        "\n",
        "            subset[name].weight.data = approx_w.data.to(torch.bfloat16)\n",
        "\n",
        "        if i == 0:\n",
        "            break\n",
        "\n",
        "    print(\"Pruning completed\")\n",
        "    return None, None\n",
        "\n",
        "def rank_reduction_weight_wrapper(args, model, tokenizer, rank_pruning, device):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "    layers_singular_value = {}\n",
        "\n",
        "    for i in tqdm(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "\n",
        "        attention = getattr(layer, 'self_attn')\n",
        "        for key, module in attention.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'self_attn.' + key\n",
        "                k = min(module.weight.shape[0], module.weight.shape[1]) - rank_pruning[i][name]\n",
        "                l = LowRankLayer(k, module.weight.to(torch.float32))\n",
        "                setattr(attention, key, l)\n",
        "                del module\n",
        "        mlp = getattr(layer, 'mlp')\n",
        "        for key, module in mlp.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'mlp.' + key\n",
        "                k = min(module.weight.shape[0], module.weight.shape[1]) - rank_pruning[i][name]\n",
        "                l = LowRankLayer(k, module.weight.clone().to(torch.float32))\n",
        "                setattr(mlp, key, l)\n",
        "                del module\n",
        "        # break\n",
        "    print(\"Pruning completed\")\n",
        "\n",
        "def rank_reduction_weight_wrapper_selective(args, model, tokenizer, rank_pruning, device):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "    layers_singular_value = {}\n",
        "    reduced_rank, total_rank = 0, 0\n",
        "    for i in tqdm(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "\n",
        "        attention = getattr(layer, 'self_attn')\n",
        "        for key, module in attention.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'self_attn.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) * 100 > 40:\n",
        "                    l = LowRankLayer(k, module.weight.to(torch.float32), False)\n",
        "                    setattr(attention, key, l)\n",
        "                    del module\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                total_rank += rank\n",
        "        mlp = getattr(layer, 'mlp')\n",
        "        for key, module in mlp.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'mlp.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) * 100 > 40:\n",
        "                    l = LowRankLayer(k, module.weight.clone().to(torch.float32), False)\n",
        "                    setattr(mlp, key, l)\n",
        "                    del module\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                total_rank += rank\n",
        "        # break\n",
        "    print(f\">>>>>>>>>>>>>>> Pruning completed with Rank reduced : {(reduced_rank/total_rank) * 100}\")\n",
        "    return (reduced_rank/total_rank) * 100\n",
        "\n",
        "\n",
        "\n",
        "def rank_reduction_weight_wrapper_selective_eval(args, model, tokenizer, rank_pruning, device):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "    layers_singular_value = {}\n",
        "    reduced_rank, total_rank = 0, 0\n",
        "    for i in tqdm(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "\n",
        "        attention = getattr(layer, 'self_attn')\n",
        "        for key, module in attention.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'self_attn.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) * 100 > 40:\n",
        "                    l = LowRankLayerEval(k, module.weight.to(torch.float32), False)\n",
        "                    setattr(attention, key, l)\n",
        "                    del module\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                total_rank += rank\n",
        "        mlp = getattr(layer, 'mlp')\n",
        "        for key, module in mlp.named_modules():\n",
        "            if \"proj\" in key:\n",
        "                name = 'mlp.' + key\n",
        "                rank = min(module.weight.shape[0], module.weight.shape[1])\n",
        "                k = rank - rank_pruning[i][name]\n",
        "                if (rank_pruning[i][name] / rank) * 100 > 40:\n",
        "                    l = LowRankLayerEval(k, module.weight.clone().to(torch.float32), False)\n",
        "                    setattr(mlp, key, l)\n",
        "                    del module\n",
        "                    reduced_rank += rank_pruning[i][name]\n",
        "                total_rank += rank\n",
        "        # break\n",
        "    print(f\">>>>>>>>>>>>>>> Pruning completed with Rank reduced : {(reduced_rank/total_rank) * 100}\")\n",
        "    return (reduced_rank/total_rank) * 100\n",
        "\n",
        "\n",
        "def rank_reduction_dynamic_pruning(args, model, device, file_name):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    rank_pruning = {}\n",
        "    total_rank, error_thresold_att, error_thresold_ffn = 0, 5e-4, 5e-4\n",
        "    pruning_bucket = [0.95, 0.9, 0.85, 0.8, 0.7, 0.75, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.2, 0.1]\n",
        "\n",
        "    for i in tqdm(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "        rank_pruning[i] = {}\n",
        "        for name in subset:\n",
        "            W = subset[name].weight.clone().data\n",
        "            if \"mlp\" in name: error_thresold = error_thresold_ffn\n",
        "            else: error_thresold = error_thresold_att\n",
        "            rank_pruning[i][name] = 0\n",
        "            for prune_ratio in pruning_bucket:\n",
        "                desired_rank = int(min(W.shape[0], W.shape[1]) * prune_ratio)\n",
        "                approx_w, error = do_low_rank(W.to(torch.float32), desired_rank, False)\n",
        "                if error > error_thresold:\n",
        "                    break\n",
        "                else:\n",
        "                    rank_pruning[i][name] = min(W.shape[0], W.shape[1]) - desired_rank\n",
        "            total_rank += int(min(W.shape[0], W.shape[1]))\n",
        "            print(f\"layer.{i}.{name} ({rank_pruning[i][name]}): {error}\")\n",
        "\n",
        "    pruned_rank = 0\n",
        "    for i in tqdm(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "        for name in subset:\n",
        "            pruned_rank += rank_pruning[i][name]\n",
        "    print(\"Pruning completed\")\n",
        "    torch.save(rank_pruning, \"/data/adative_rank_attention_ffn.pt\")\n",
        "    print(f\"Rank Reduction: {(pruned_rank/total_rank)* 100:.3f} %\", file=file_name, flush=True)\n",
        "    return rank_pruning"
      ],
      "metadata": {
        "id": "hcf_hwWghwRH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### eval."
      ],
      "metadata": {
        "id": "fprRnKQxh_eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm as tqdm\n",
        "from loguru import logger\n",
        "# Import get_loaders function from data module within the same directory\n",
        "#from .data_utils import get_c4, get_wikitext2\n",
        "\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, tokenizer)\n",
        "    if \"c4\" in name:\n",
        "        return get_c4(nsamples, seed, seqlen, tokenizer)\n",
        "\n",
        "\n",
        "# Function to evaluate perplexity (ppl) on a specified model and tokenizer\n",
        "def eval_ppl(model, tokenizer, device=torch.device(\"cuda:0\"), dataset=\"wikitext2\"):\n",
        "    # Set dataset\n",
        "\n",
        "    # Print status\n",
        "    logger.info(f\"Evaluating on {dataset} .....\")\n",
        "\n",
        "    if os.path.exists(\"./data/test_loader.pt\"):\n",
        "        testloader = torch.load(\"./data/test_loader.pt\")\n",
        "    else:\n",
        "        # Get the test loader\n",
        "        _, testloader = get_loaders(\n",
        "            dataset, seed=0, seqlen=model.seqlen, tokenizer=tokenizer\n",
        "        )\n",
        "        torch.save(testloader, \"./data/test_loader.pt\")\n",
        "\n",
        "    # Evaluate ppl in no grad context to avoid updating the model\n",
        "    with torch.no_grad():\n",
        "        ppl = eval_ppl_dataset(model, testloader, 1, device)\n",
        "    return ppl\n",
        "\n",
        "# Function to evaluate perplexity (ppl) specifically on the wikitext dataset\n",
        "def eval_ppl_dataset(model, testenc, bs=1, device=None):\n",
        "    # Get input IDs\n",
        "    testenc = testenc.input_ids\n",
        "\n",
        "    # Calculate number of samples\n",
        "    nsamples = testenc.numel() // model.seqlen\n",
        "\n",
        "    # List to store negative log likelihoods\n",
        "    nlls = []\n",
        "\n",
        "    # nsamples = 10 #Sanity check\n",
        "\n",
        "    # Loop through each batch\n",
        "    for i in range(0, nsamples, bs):\n",
        "\n",
        "        # Calculate end index\n",
        "        j = min(i+bs, nsamples)\n",
        "\n",
        "        # Prepare inputs and move to device\n",
        "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].cuda()\n",
        "        inputs = inputs.reshape(j-i, model.seqlen)\n",
        "\n",
        "        s_time = time.time()\n",
        "        # Forward pass through the model\n",
        "        lm_logits = model(inputs).logits\n",
        "        e_time = time.time()\n",
        "\n",
        "        # Shift logits and labels for next token prediction\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = inputs[:, 1:]\n",
        "\n",
        "        # Compute loss\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "\n",
        "        # Calculate negative log likelihood\n",
        "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
        "\n",
        "        # Append to list of negative log likelihoods\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "        if i % 20 == 0: logger.info(f\"Evaluated samples: {i}/{nsamples}\")\n",
        "\n",
        "    # Compute perplexity\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
        "\n",
        "    # print(ppl)\n",
        "    # Empty CUDA cache to save memory\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "    return ppl.item()"
      ],
      "metadata": {
        "id": "8yeedx65iAZr"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}